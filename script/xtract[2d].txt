#!/usr/bin/env python3
import re
import time
import os
import requests
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urlparse, unquote
import math

def clean_sheet_name(name):
    """Clean invalid characters from Excel sheet names"""
    name = re.sub(r'[\\/*?:\[\]]', '', name)
    return name[:31]  # Excel limit is 31 characters

def generate_filename(url, index, format='xlsx'):
    """Generate filename from URL or use incremental number"""
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.replace('www.', '').split('.')[0]
        path = unquote(parsed.path).replace('/', '_').strip('_')[:20]
        
        if domain and path:
            return f"{domain}_{path}.{format}"
        elif domain:
            return f"{domain}.{format}"
    except:
        pass
    return f"{index}.{format}"

def manual_table_extraction(table):
    """Manually extract table data from BeautifulSoup table object"""
    headers = []
    thead = table.find('thead')
    if thead:
        for th in thead.find_all('th'):
            headers.append(th.get_text(strip=True))
    
    rows = []
    tbody = table.find('tbody')
    if tbody:
        for row in tbody.find_all('tr'):
            cols = row.find_all('td')
            if cols:  # Skip empty rows
                row_data = [col.get_text(strip=True) for col in cols]
                rows.append(row_data)
    
    if headers and rows:
        return pd.DataFrame(rows, columns=headers)
    elif rows:
        return pd.DataFrame(rows)
    else:
        return None

def extract_tables_robust(url, table_class=None):
    """Reliable table extraction that works for complex sites"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        if table_class:
            tables = soup.find_all('table', class_=table_class)
        else:
            tables = soup.find_all('table')
            
        return tables
    except Exception as e:
        print(f"âš ï¸ Error fetching {url}: {str(e)}")
        return []

def extract_url(url, table_class=None, num_pages=1):
    """Extract tables from a single URL with pagination support"""
    all_tables = []
    page_range = range(1, num_pages + 1) if "{}" in url else [1]
    
    print(f"â³ Extracting tables from {min(page_range)} to {max(page_range)} pages...")
    
    for page in page_range:
        try:
            target_url = url.format(page) if "{}" in url else url
            print(f"  ðŸŒ Fetching page {page}: {target_url}")
            
            tables = extract_tables_robust(target_url, table_class)
            if not tables:
                print(f"    âš ï¸ No tables found on page {page}")
                continue
            
            for i, table in enumerate(tables):
                df = manual_table_extraction(table)
                if df is None or df.empty:
                    print(f"    âš ï¸ Table {i+1} is empty")
                    continue
                
                # Add metadata
                df['Source_URL'] = target_url
                df['Table_Index'] = i + 1
                if len(page_range) > 1:
                    df['Page_Number'] = page
                
                all_tables.append(df)
                print(f"    âœ… Extracted table {i+1} ({len(df)} rows)")
                
            time.sleep(1.5)  # Be polite to servers
            
        except Exception as e:
            print(f"ðŸš¨ Page {page} failed: {str(e)}")
    
    return all_tables

def save_tables(tables, output_path, table_class=None, source_url=None):
    """Save extracted tables to appropriate format"""
    if not tables:
        print("âŒ No tables to save")
        return False
    
    if output_path.endswith('.csv'):
        combined = pd.concat(tables, ignore_index=True)
        combined.to_csv(output_path, index=False)
        print(f"ðŸ’¾ Saved CSV: {os.path.abspath(output_path)}")
        return True
    
    elif output_path.endswith('.xlsx'):
        try:
            with pd.ExcelWriter(output_path) as writer:
                # Single sheet for classified tables
                if table_class:
                    combined = pd.concat(tables, ignore_index=True)
                    combined.to_excel(writer, sheet_name="Data", index=False)
                    print(f"ðŸ’¾ Saved Excel with combined sheet: {os.path.abspath(output_path)}")
                # Multiple sheets for general extraction
                else:
                    for idx, df in enumerate(tables):
                        # Create meaningful sheet names
                        if source_url:
                            domain = urlparse(source_url).netloc.replace('www.', '')[:15]
                            sheet_name = f"{domain}_T{idx+1}"
                        else:
                            sheet_name = f"Table{idx+1}"
                            
                        sheet_name = clean_sheet_name(sheet_name)
                        df.to_excel(writer, sheet_name=sheet_name, index=False)
                    print(f"ðŸ’¾ Saved Excel with {len(tables)} sheets: {os.path.abspath(output_path)}")
            return True
        except Exception as e:
            print(f"ðŸš¨ Excel save failed: {str(e)}")
            return False
    
    else:
        print("âŒ Unsupported format. Using default: output.xlsx")
        with pd.ExcelWriter("output.xlsx") as writer:
            pd.concat(tables).to_excel(writer, index=False)
        return True

def main():
    print("\n" + "="*50)
    print("ðŸŒ XTRACT - WEB TABLE EXTRACTOR".center(50))
    print("="*50)
    
    # Single vs Multiple URLs
    mode = input("\nSingle URL or Multiple URLs? (1/2): ").strip()
    
    # Process URLs based on mode
    urls = []
    if mode == '1':
        # Single URL processing
        url_input = input("\nEnter website URL:\n> ").strip()
        
        # URL logic handling
        if re.search(r'/\d+$', url_input):  # Logic 1: Specific page
            url = url_input
            num_pages = 1
        elif re.search(r'\{(\d+)\}', url_input):  # Logic 2: Page range
            num_pages = int(re.search(r'\{(\d+)\}', url_input).group(1))
            url = re.sub(r'\{\d+\}', '{}', url_input)
        else:  # Logic 3: Single page
            url = url_input
            num_pages = 1
            
        urls.append((url, num_pages))
        
        # Table class handling
        table_class = input("\nEnter table class (optional, press Enter to extract all tables):\n> ").strip() or None
        if not table_class:
            print("\nâš ï¸ Extracting ALL tables. This may include non-data tables")
        
        # Output filename
        default_name = generate_filename(url, 0)
        filename = input(f"\nEnter output filename (default: {default_name}):\n> ").strip() or default_name
        
        # Extract and save
        tables = extract_url(url, table_class, num_pages)
        if tables:
            save_tables(tables, filename, table_class, url)
        
    elif mode == '2':
        # Multiple URL processing
        urls_input = input("\nEnter URLs (space/comma separated):\n> ").strip()
        
        # Split URLs using both spaces and commas
        raw_urls = re.split(r'[,\s]+', urls_input)
        urls = []
        
        # Process each URL
        for url in raw_urls:
            if not url:
                continue
                
            if re.search(r'/\d+$', url):  # Specific page
                urls.append((url, 1))
            elif re.search(r'\{(\d+)\}', url):  # Page range
                num_pages = int(re.search(r'\{(\d+)\}', url).group(1))
                base_url = re.sub(r'\{\d+\}', '{}', url)
                urls.append((base_url, num_pages))
            else:  # Single page
                urls.append((url, 1))
        
        print(f"\nFound {len(urls)} URLs to process")
        
        # Output handling
        output_mode = input("\nSave in single file or multiple files? (1/2): ").strip()
        
        if output_mode == '1':  # Single file
            filename = input("\nEnter output filename (must be .xlsx):\n> ").strip()
            if not filename.endswith('.xlsx'):
                filename += '.xlsx'
                
            all_tables = []
            for i, (url, num_pages) in enumerate(urls):
                print(f"\nProcessing URL {i+1}/{len(urls)}: {url}")
                tables = extract_url(url, None, num_pages)
                for table in tables:
                    table['URL_Index'] = i + 1
                    table['Source_URL'] = url.replace('{}', str(num_pages)) if "{}" in url else url
                all_tables.extend(tables)
                
            if save_tables(all_tables, filename):
                print("\n" + "="*50)
                print("ðŸ’¡ To merge sheets in Excel:")
                print("1. Open the Excel file")
                print("2. Press Alt+F11 to open VBA editor")
                print("3. Insert > Module and paste this code:")
                print("""
Sub UnmergeAndConsolidateSheets()
    Dim ws As Worksheet
    Dim newWs As Worksheet
    Dim lastRow As Long
    Dim destRow As Long
    
    On Error Resume Next
    Set newWs = ThisWorkbook.Sheets("Consolidated Data")
    On Error GoTo 0
    
    If Not newWs Is Nothing Then
        Application.DisplayAlerts = False
        newWs.Delete
        Application.DisplayAlerts = True
    End If
    
    Set newWs = ThisWorkbook.Sheets.Add
    newWs.Name = "Consolidated Data"
    destRow = 1
    
    For Each ws In ThisWorkbook.Sheets
        If ws.Name <> newWs.Name Then
            ws.Cells.UnMerge
            lastRow = ws.Cells(ws.Rows.Count, "A").End(xlUp).Row
            If lastRow > 1 Then
                ws.Rows("2:" & lastRow).Copy newWs.Cells(destRow, 1)
                destRow = destRow + (lastRow - 1)
            End If
        End If
    Next ws
    
    newWs.Cells.EntireColumn.AutoFit
    Set newWs = Nothing
End Sub
                """)
                print("4. Press F5 to run the macro")
                print("="*50)
                
        else:  # Multiple files
            output_dir = input("\nEnter output directory (default: current directory):\n> ").strip()
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir)
                
            for i, (url, num_pages) in enumerate(urls):
                print(f"\nProcessing URL {i+1}/{len(urls)}: {url}")
                tables = extract_url(url, None, num_pages)
                
                if tables:
                    filename = generate_filename(url, i)
                    output_path = os.path.join(output_dir, filename) if output_dir else filename
                    save_tables(tables, output_path, None, url)
    
    print("\nâœ… Extraction complete! Happy analyzing!")
    print("="*50)

if __name__ == "__main__":
    main()
